{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "import json\n",
    "import pickle\n",
    "import torch\n",
    "import glob\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split\n",
    "import time\n",
    "import networkx\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import scipy.io as sio\n",
    "import argparse\n",
    "from models import Model"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "import torch_geometric as geo\n",
    "\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"#, 1, 2\"\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def visualize(h, color):\n",
    "    z = TSNE(n_components=2).fit_transform(out.detach().cpu().numpy())\n",
    "\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "\n",
    "    plt.scatter(z[:, 0], z[:, 1], s=70, c=color, cmap=\"Set2\")\n",
    "    plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "dataset_name = 'PROTEINS'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "def getDataset(root, name, transform):\n",
    "    if name.lower() in ['cora', 'pubmed', 'citeseer']:\n",
    "        dataset = geo.datasets.Planetoid(root=root, name=name, transform=transform)\n",
    "    elif name.lower() in ['mutag', 'imdb-binary', 'ethanol', 'proteins']:\n",
    "        dataset =geo.datasets.TUDataset(root=root, name=name, transform=transform,use_node_attr=True)\n",
    "    else:\n",
    "        raise NotImplementedError(\"{} not supported!\".format(name))\n",
    "    return dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "dataset = getDataset('data', dataset_name, None)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('======================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "print('===========================================================================================================')\n",
    "\n",
    "# Gather some statistics about the graph.\n",
    "print(f'Number of nodes: {data.num_nodes}')\n",
    "print(f'Number of edges: {data.num_edges}')\n",
    "print(f'Average node degree: {data.num_edges / data.num_nodes:.2f}')\n",
    "print(f'Contains isolated nodes: {data.contains_isolated_nodes()}')\n",
    "print(f'Contains self-loops: {data.contains_self_loops()}')\n",
    "print(f'Is undirected: {data.is_undirected()}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "Dataset: PROTEINS(1113):\n",
      "======================\n",
      "Number of graphs: 1113\n",
      "Number of features: 4\n",
      "Number of classes: 2\n",
      "\n",
      "Data(edge_index=[2, 162], x=[42, 4], y=[1])\n",
      "===========================================================================================================\n",
      "Number of nodes: 42\n",
      "Number of edges: 162\n",
      "Average node degree: 3.86\n",
      "Contains isolated nodes: False\n",
      "Contains self-loops: False\n",
      "Is undirected: True\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## define model architecture "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "if dataset_name == 'PROTEINS':\n",
    "    print(\"USING Hierarchical Graph Pooling with Structure Learning\")\n",
    "    with open('config-{}.pickle'.format(dataset_name), 'rb') as handle:\n",
    "        args = pickle.load(handle)\n",
    "    args.device = 'cpu'\n",
    "    print(args)\n",
    "    num_training = int(len(dataset) * 0.8)\n",
    "    num_val = int(len(dataset) * 0.1)\n",
    "    num_test = len(dataset) - (num_training + num_val)\n",
    "    training_set, validation_set, test_set = random_split(dataset, [num_training, num_val, num_test])\n",
    "    \n",
    "    train_loader = geo.data.DataLoader(training_set, batch_size=args.batch_size, shuffle=True)\n",
    "    val_loader = geo.data.DataLoader(validation_set, batch_size=args.batch_size, shuffle=False)\n",
    "    test_loader = geo.data.DataLoader(test_set, batch_size=args.batch_size, shuffle=False)\n",
    "    try:\n",
    "        model = Model(args).to(args.device)\n",
    "    except RuntimeError:\n",
    "        args.device = 'cpu'\n",
    "        model = Model(args).to(args.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    \n",
    "    def train():\n",
    "        min_loss = 1e10\n",
    "        patience_cnt = 0\n",
    "        val_loss_values = []\n",
    "        best_epoch = 0\n",
    "\n",
    "        t = time.time()\n",
    "        model.train()\n",
    "        for epoch in range(args.epochs):\n",
    "            loss_train = 0.0\n",
    "            correct = 0\n",
    "            for i, data in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "                data = data.to(args.device)\n",
    "                out = model(data)\n",
    "                loss = F.nll_loss(out, data.y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                loss_train += loss.item()\n",
    "                pred = out.max(dim=1)[1]\n",
    "                correct += pred.eq(data.y).sum().item()\n",
    "            acc_train = correct / len(train_loader.dataset)\n",
    "            acc_val, loss_val = compute_test(val_loader)\n",
    "            print('Epoch: {:04d}'.format(epoch + 1), 'loss_train: {:.6f}'.format(loss_train),\n",
    "                  'acc_train: {:.6f}'.format(acc_train), 'loss_val: {:.6f}'.format(loss_val),\n",
    "                  'acc_val: {:.6f}'.format(acc_val), 'time: {:.6f}s'.format(time.time() - t))\n",
    "\n",
    "            val_loss_values.append(loss_val)\n",
    "            torch.save(model.state_dict(), 'models/PROTEINS/{}.pth'.format(epoch))\n",
    "            if val_loss_values[-1] < min_loss:\n",
    "                min_loss = val_loss_values[-1]\n",
    "                best_epoch = epoch\n",
    "                patience_cnt = 0\n",
    "            else:\n",
    "                patience_cnt += 1\n",
    "\n",
    "            if patience_cnt == args.patience:\n",
    "                break\n",
    "\n",
    "            files = glob.glob('models/PROTEINS/*.pth')\n",
    "            for f in files:\n",
    "                epoch_nb = int(f.split('/')[-1].split('.')[0])\n",
    "                if epoch_nb < best_epoch:\n",
    "                    os.remove(f)\n",
    "\n",
    "        files = glob.glob('models/PROTEINS/*.pth')\n",
    "        for f in files:\n",
    "            epoch_nb = int(f.split('/')[-1].split('.')[0])\n",
    "            if epoch_nb > best_epoch:\n",
    "                os.remove(f)\n",
    "        print('Optimization Finished! Total time elapsed: {:.6f}'.format(time.time() - t))\n",
    "\n",
    "        return best_epoch\n",
    "\n",
    "\n",
    "    def compute_test(loader):\n",
    "        model.eval()\n",
    "        correct = 0.0\n",
    "        loss_test = 0.0\n",
    "        for data in loader:\n",
    "            data = data.to(args.device)\n",
    "            out = model(data)\n",
    "            pred = out.max(dim=1)[1]\n",
    "            correct += pred.eq(data.y).sum().item()\n",
    "            loss_test += F.nll_loss(out, data.y).item()\n",
    "        return correct / len(loader.dataset), loss_test\n",
    "    \n",
    "else:\n",
    "    class GCN_node(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super(GCN_node, self).__init__()\n",
    "            torch.manual_seed(0)\n",
    "            self.conv1 = geo.nn.GCNConv(dataset.num_features, hidden_channels)\n",
    "            self.conv2 = geo.nn.GCNConv(hidden_channels, dataset.num_classes)\n",
    "\n",
    "        def forward(self, x, edge_index):\n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = nn.functional.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.conv2(x, edge_index)\n",
    "            return x\n",
    "\n",
    "    class GCN_graph(torch.nn.Module):\n",
    "        def __init__(self, hidden_channels):\n",
    "            super(GCN_graph, self).__init__()\n",
    "            torch.manual_seed(0)\n",
    "            self.conv1 = geo.nn.GCNConv(dataset.num_node_features, hidden_channels)\n",
    "            self.conv2 = geo.nn.GCNConv(hidden_channels, hidden_channels)\n",
    "            self.conv3 = geo.nn.GCNConv(hidden_channels, hidden_channels)\n",
    "            self.lin = nn.Linear(hidden_channels, dataset.num_classes)\n",
    "\n",
    "        def forward(self, x, edge_index, batch):\n",
    "            # not using edge attributes as it's hard to fake\n",
    "            # 1. Obtain node embeddings \n",
    "            x = self.conv1(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = x.relu()\n",
    "            x = self.conv3(x, edge_index)\n",
    "\n",
    "            # 2. Readout layer\n",
    "            x = geo.nn.global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "            # 3. Apply a final classifier\n",
    "            x = nn.functional.dropout(x, p=0.5, training=self.training)\n",
    "            x = self.lin(x)\n",
    "\n",
    "            return x\n",
    "    device = torch.device('cuda')\n",
    "    victim_model = GCN_graph(hidden_channels=512).to(device)\n",
    "    optimizer = torch.optim.Adam(victim_model.parameters(), lr=0.01)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def train():\n",
    "        victim_model.train()\n",
    "\n",
    "        for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "            data = data.to(device)\n",
    "            out = victim_model(data.x, data.edge_index, data.batch)  # Perform a single forward pass.\n",
    "            loss = criterion(out, data.y)  # Compute the loss.\n",
    "            loss.backward()  # Derive gradients.\n",
    "            optimizer.step()  # Update parameters based on gradients.\n",
    "            optimizer.zero_grad()  # Clear gradients.\n",
    "\n",
    "    def test(loader):\n",
    "        victim_model.eval()\n",
    "        correct = 0\n",
    "        for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "            data = data.to(device)\n",
    "            out = victim_model(data.x, data.edge_index, data.batch)  \n",
    "            pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "            correct += int((pred == data.y).sum())  # Check against ground-truth labels.\n",
    "        return correct / len(loader.dataset)  # Derive ratio of correct predictions."
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "USING Hierarchical Graph Pooling with Structure Learning\n",
      "Namespace(batch_size=512, dataset='PROTEINS', device='cpu', dropout_ratio=0.0, epochs=1000, lamb=1.0, lr=0.001, nhid=128, num_classes=2, num_features=4, patience=100, pooling_ratio=0.5, sample_neighbor=True, seed=777, sparse_attention=True, structure_learning=True, weight_decay=0.001)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "trained = True"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## freeze model parameters and make it deterministic"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "if not trained:\n",
    "    if dataset_name != \"PROTEINS\":\n",
    "        for epoch in range(1, 501):\n",
    "            train()\n",
    "            if epoch % 20 == 0:\n",
    "                train_acc = test(train_loader)\n",
    "                test_acc = test(test_loader)\n",
    "                print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}')\n",
    "    else:\n",
    "        # Model training\n",
    "        best_model = train()\n",
    "        # Restore best model for test set\n",
    "        model.load_state_dict(torch.load('models/PROTEINS/{}.pth'.format(best_model)))\n",
    "        test_acc, test_loss = compute_test(test_loader)\n",
    "        print('Test set results, loss = {:.6f}, accuracy = {:.6f}'.format(test_loss, test_acc))\n",
    "else:\n",
    "    model.load_state_dict(torch.load('models/PROTEINS/220.pth', map_location=args.device))\n",
    "    model.eval()\n",
    "    test_acc, test_loss = compute_test(test_loader)\n",
    "    print('Test set results, loss = {:.6f}, accuracy = {:.6f}'.format(test_loss, test_acc))\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = False"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Test set results, loss = 0.477780, accuracy = 0.812500\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## check all possible candidates (assume prior knowledge)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "cands = {}\n",
    "for i in range(len(dataset)):\n",
    "    for j in range(dataset[i].x.shape[0]):\n",
    "        key = '{} {}'.format(dataset[i].x[j, 0].item(), dataset[i].x[j, 1:].argmax().item())\n",
    "        try:\n",
    "            cands[key] += 1\n",
    "        except KeyError:\n",
    "            cands[key] = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## load generated class impressions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "cldataList = []\n",
    "for i in range(2):\n",
    "    label = torch.Tensor([i]).long()\n",
    "    for fin in os.listdir(os.path.join('data', dataset_name, 'classImpression', str(i)+'_tropology')):\n",
    "        tmp = torch.load(os.path.join('data', dataset_name, 'classImpression', str(i)+'_tropology', fin))\n",
    "        tmp.y = label\n",
    "        tmp = geo.data.Data(x=tmp.x, edge_index=tmp.edge_index, edge_attr=tmp.edge_attr, y=tmp.y)\n",
    "        cldataList.append(tmp.to(args.device))\n",
    "        if (tmp.edge_index.max().item() - tmp.x.shape[0]) != -1:\n",
    "            print(fin, i)\n",
    "        if (tmp.edge_index.shape[1] != tmp.edge_attr.shape[0]):\n",
    "            print(fin, i)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "class surrogateData(geo.data.Dataset):\n",
    "    def __init__(self, dataList):\n",
    "        super().__init__()\n",
    "        self.data = dataList\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        self.data[index].x.long\n",
    "        return self.data[index]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "surData = surrogateData(cldataList)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "source": [
    "print('======================')\n",
    "print(f'Number of graphs: {len(surData)}')\n",
    "print(f'Number of features: {surData.num_features}')\n",
    "# print(f'Number of classes: {surData.num_classes}')\n",
    "num_nodes = 0\n",
    "num_edges = 0\n",
    "for i in range(len(surData)):\n",
    "    data = surData[i]  # Get the first graph object.\n",
    "    num_nodes += data.num_nodes\n",
    "    num_edges += (data.edge_attr>=0.98).sum()\n",
    "print(num_edges.item()/num_nodes)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "======================\n",
      "Number of graphs: 677\n",
      "Number of features: 4\n",
      "45.92490515454739\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "source": [
    "idx2adj(data)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 0., 1.,  ..., 1., 1., 1.],\n",
       "        [1., 1., 0.,  ..., 1., 1., 1.],\n",
       "        ...,\n",
       "        [1., 1., 1.,  ..., 0., 1., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 0., 1.],\n",
       "        [1., 1., 1.,  ..., 1., 1., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 69
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "source": [
    "data.edge_index[:, data.edge_attr >= 0.98]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[  2,   3,   3,  ..., 304, 305, 306],\n",
       "        [  0,   1,   2,  ...,   0,   0,   0]])"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## randomly initialize a node as our universal trigger (previous approach)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "def getNodes(n):    \n",
    "    # node features are 4D vectors, the first dimension means the van de wall force and the next 3 are one-hot-encoded category\n",
    "    tmp = torch.cat((torch.randint(-500, 800, (n,1), device=args.device),\n",
    "                     torch.nn.functional.one_hot(torch.randint(0, 3, (n,), device=args.device), num_classes=3)), dim=1)\n",
    "    return tmp.float().clone()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "def getCands(n, idx=None):\n",
    "    # get n nodes based on candidate set, idx is the index of node feature\n",
    "    if not idx:\n",
    "        idxs = random.choices(list(range(len(list(cands.keys())))), k=n)\n",
    "    else:\n",
    "        idxs = idx\n",
    "    dataList = []\n",
    "    for i in idxs:\n",
    "        vandewallF, encoded = list(cands.keys())[i].split()\n",
    "        dataList.append(torch.cat((torch.as_tensor([float(vandewallF)], device=args.device).long(), \n",
    "                                   F.one_hot(torch.as_tensor([int(encoded)], device=args.device).long(), num_classes=3).squeeze())).unsqueeze(0))\n",
    "    return torch.cat(dataList, dim=0).float().clone()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "def getTrigger(n, idx=None, weighted=False):\n",
    "    # idx is the list of required indecies\n",
    "    if idx:\n",
    "        assert n == len(idx)\n",
    "    trigger = geo.data.Data()\n",
    "    trigger.x = getCands(n, idx)\n",
    "    adv_adj = torch.zeros(size=(n, n)).bool()\n",
    "    for i in range(n):\n",
    "        adv_adj[i, i:].random_(0, 2)\n",
    "    adv_adj = adv_adj.int()\n",
    "    for i in range(n):\n",
    "        for j in range(i, n):\n",
    "            adv_adj[j, i] = adv_adj[i, j]\n",
    "    if n > 1:\n",
    "        try:\n",
    "            trigger.edge_index, _ = geo.utils.remove_self_loops(adj2idx(adv_adj).long())\n",
    "        except RuntimeError:\n",
    "            # in case of no edge generated\n",
    "            return getTrigger(n, idx, weighted)\n",
    "        if geo.utils.contains_isolated_nodes(trigger.edge_index, num_nodes=n):\n",
    "            return getTrigger(n, idx, weighted)\n",
    "        else:\n",
    "            if weighted:\n",
    "                trigger.edge_attr = torch.ones((trigger.edge_index.shape[1], ))\n",
    "            return trigger\n",
    "    else:\n",
    "        return trigger"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "def idx2adj(data):\n",
    "    device = data.edge_index.device\n",
    "    edge_index = torch.zeros(size=(data.num_nodes, data.num_nodes), device=device)\n",
    "    for i in range(data.edge_index.shape[1]):\n",
    "        edge_index[data.edge_index[0][i]][data.edge_index[1][i]] = 1\n",
    "    return edge_index\n",
    "\n",
    "def adj2idx(edge_index):\n",
    "    assert edge_index.shape[0] == edge_index.shape[1]\n",
    "    tmp = []\n",
    "    for i in range(edge_index.shape[0]):\n",
    "        for j in range(edge_index.shape[0]):\n",
    "            if edge_index[i][j] == 1:\n",
    "                tmp.append([i, j])\n",
    "    return torch.Tensor(tmp).permute(1,0).to(edge_index.device)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "def append(ori_graph, appendix, anchor_pos):\n",
    "    # function to append a group of nodes to original graph\n",
    "    idx_anchor = (ori_graph.edge_index[0] > anchor_pos).int().argmin() + 1\n",
    "    first_half = ori_graph.edge_index[:, :idx_anchor]\n",
    "    second_half = ori_graph.edge_index[:, idx_anchor + 1:]\n",
    "    new_edge0 = torch.as_tensor([[anchor_pos], [ori_graph.num_nodes]], \n",
    "                             device=ori_graph.x.device).long()\n",
    "    new_edge1 = torch.as_tensor([[ori_graph.num_nodes], [anchor_pos]],\n",
    "                             device=ori_graph.x.device).long()\n",
    "    if appendix.num_nodes > 1:\n",
    "        new_edge_index = torch.cat([first_half, new_edge0, \n",
    "                                    second_half, new_edge1,\n",
    "                                    appendix.edge_index + ori_graph.num_nodes],\n",
    "                                   dim=1)\n",
    "    else:\n",
    "        new_edge_index = torch.cat([first_half, new_edge0, second_half, new_edge1], dim=1)\n",
    "    new_x = torch.cat([ori_graph.x, appendix.x], dim=0)\n",
    "    ori_graph.edge_index = new_edge_index\n",
    "    ori_graph.x = new_x\n",
    "    ori_graph.num_nodes += appendix.num_nodes\n",
    "    if ori_graph.edge_attr is not None:\n",
    "#         print(\"Appending to a weighted graph\")\n",
    "        first_half = ori_graph.edge_attr[:idx_anchor]\n",
    "        second_half = ori_graph.edge_attr[idx_anchor + 1:]\n",
    "        new_edge0 = torch.as_tensor([1.0], device=ori_graph.x.device)\n",
    "        new_edge1 = torch.as_tensor([1.0], device=ori_graph.x.device)\n",
    "        if appendix.num_nodes > 1:\n",
    "            edge_attr = torch.cat([first_half, new_edge0, second_half, new_edge1, appendix.edge_attr])\n",
    "        else:\n",
    "            edge_attr = torch.cat([first_half, new_edge0, second_half, new_edge1])\n",
    "        ori_graph.edge_attr = edge_attr\n",
    "    return ori_graph"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "def batch_from_list(dataList):\n",
    "    # generate a batched data from a list of geometric data\n",
    "    # using this since wrong number of batch for surrogate data\n",
    "    outp = geo.data.Batch()\n",
    "    data = dataList[0]\n",
    "    keys = data.keys\n",
    "    keys.append('batch')\n",
    "    batchedData = {key: [] for key in keys}\n",
    "    cnt = 0\n",
    "    prev_nodes = 0\n",
    "    for data in dataList:\n",
    "        for key in data.keys:\n",
    "            if key == 'edge_index':\n",
    "                data[key] += prev_nodes\n",
    "            batchedData[key].append(data[key])\n",
    "        batchedData['batch'].append(torch.ones((data.x.shape[0], ), dtype=torch.int64) * cnt)\n",
    "        prev_nodes += data.x.shape[0]\n",
    "        cnt += 1\n",
    "    for key in keys:\n",
    "        if key == 'edge_index':\n",
    "            catDim = -1\n",
    "        else:\n",
    "            catDim = 0\n",
    "        outp[key] = torch.cat(batchedData[key], dim=catDim)\n",
    "    return outp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def BatchAppend(ori_graphs, trigger, pos_mode='deg'):\n",
    "    # taking list of graphs, graph as input, return batch of graphes\n",
    "    modi_graphs = []\n",
    "    for i in range(len(ori_graphs)):\n",
    "        tmp = ori_graphs[i].clone()\n",
    "        pos_mode = pos_mode.lower()\n",
    "        if pos_mode == 'deg':\n",
    "            glueLoc = geo.utils.degree(tmp.edge_index[0]).argmax() # append to node with the highest degree\n",
    "        elif pos_mode == 'min':\n",
    "            # for experiment only, every other centrality measures take the highest value\n",
    "            glueLoc = geo.utils.degree(tmp.edge_index[0]).argmin() # append to node with the lowest degree\n",
    "        elif pos_mode == 'eig':\n",
    "            # eigen vector centrality measures\n",
    "            G = geo.utils.to_networkx(tmp)\n",
    "            eigen_centrality = networkx.algorithms.centrality.eigenvector_centrality_numpy(G)\n",
    "            glueLoc = sorted(eigen_centrality, key=eigen_centrality.get, reverse=True)[0]\n",
    "        elif pos_mode == 'btw':\n",
    "            # betweenness centrality measures\n",
    "            G = geo.utils.to_networkx(tmp)\n",
    "            btw_centrality = networkx.algorithms.centrality.betweenness_centrality(G)\n",
    "            glueLoc = sorted(btw_centrality, key=btw_centrality.get, reverse=True)[0]\n",
    "        else:\n",
    "            raise NotImplementedError(\"Only degree, eigen-vector, and betweenness centrality measures!\")\n",
    "        modi_graph = append(tmp, trigger, glueLoc)\n",
    "        modi_graphs.append(modi_graph)\n",
    "#         print(modi_graph, modi_graphs)\n",
    "#     modi_graphs = geo.data.Batch.from_data_list(modi_graphs).to(args.device)\n",
    "    modi_graphs = batch_from_list(modi_graphs)\n",
    "    return modi_graphs"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## generate list of all possible triggers"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "triggerList = []\n",
    "for i in range(len(cands.keys())):\n",
    "    triggerList.append(getTrigger(1, [i]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## adopt hot-flip universal adversarial attack\n",
    "1. Generate candidates for universal trigger, it should contains all feasible nodes\n",
    "2. compute replace nodes in triggers by argmin(e_i - e_adv_i)^T \\nabla_e_adv_i loss (note that embedding can be extracted from model)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### question: how to get individual embedding? \n",
    "* difference by dropping it?\n",
    "* different edges will result in different embedding, how to solve it?\n",
    "* current solution: limit trigger length to 1 (bit like an exhaustive search?)\n",
    "* single node embedding by difference of graph embeddings\n",
    "* gradient of node embedding wrt loss by difference of gradients of graph embeddings wrt losses"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "def getOutput(embed):\n",
    "    outp = F.relu(model.lin1(embed))\n",
    "    outp = F.dropout(outp, p=model.dropout_ratio, training=model.training)\n",
    "    outp = F.relu(model.lin2(outp))\n",
    "    outp = F.dropout(outp, p=model.dropout_ratio, training=model.training)\n",
    "    outp = F.log_softmax(model.lin3(outp), dim=-1)\n",
    "    return outp"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## sample code for non-batched graph UAA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "# modi_graph = BatchAppend([dataset[0]], trigger)\n",
    "\n",
    "# modi_graph.x.requires_grad = True\n",
    "\n",
    "# embed_adv = model(modi_graph)\n",
    "\n",
    "# ori_graph = geo.data.Batch.from_data_list([dataset[0]]).to(args.device)\n",
    "# ori_graph.x.requires_grad = True\n",
    "\n",
    "# embed_ori = model(ori_graph)\n",
    "# embed_others = model(BatchAppend([dataset[0]], getTrigger(1)).to(args.device))\n",
    "\n",
    "# output = getOutput(embed_adv)\n",
    "# loss = F.nll_loss(output, dataset[0].y)\n",
    "\n",
    "# grad_embed_adv = torch.autograd.grad(loss, embed_adv)#, allow_unused=True)\n",
    "\n",
    "# output = getOutput(embed_ori)\n",
    "# loss = F.nll_loss(output, dataset[0].y)\n",
    "# grad_embed_ori = torch.autograd.grad(loss, embed_ori)#, allow_unused=True)\n",
    "\n",
    "# torch.matmul(grad_embed_ori[0] - grad_embed_adv[0], (embed_others - embed_adv).T)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## sample code for batched graph UAA"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# modi_graph = BatchAppend(dataset[:8], trigger)\n",
    "\n",
    "# modi_graph.x.requires_grad = True\n",
    "\n",
    "# embed_adv = model(modi_graph)\n",
    "\n",
    "# ori_graph = geo.data.Batch.from_data_list(dataset[:8]).to(args.device)\n",
    "# ori_graph.x.requires_grad = True\n",
    "\n",
    "# embed_ori = model(ori_graph)\n",
    "# embed_others = model(BatchAppend(dataset[:8], getTrigger(1)).to(args.device))\n",
    "\n",
    "# labels = geo.data.Batch.from_data_list(dataset[:8]).y\n",
    "# output = getOutput(embed_adv)\n",
    "# loss = F.nll_loss(output, labels)\n",
    "\n",
    "# grad_embed_adv = torch.autograd.grad(loss, embed_adv)#, allow_unused=True)\n",
    "\n",
    "# output = getOutput(embed_ori)\n",
    "# loss = F.nll_loss(output, labels)\n",
    "# grad_embed_ori = torch.autograd.grad(loss, embed_ori)#, allow_unused=True)\n",
    "\n",
    "# torch.matmul((embed_others - embed_adv).mean(dim=0).unsqueeze(0), (grad_embed_ori[0] - grad_embed_adv[0]).mean(dim=0).unsqueeze(-1)).item()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## generate universal trigger via graph embedding difference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "source": [
    "batch_size = 1"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "result = {}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "pos_modes = ['deg', 'min', 'btw', 'eig']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "for pos_mode in pos_modes:\n",
    "    log = {}\n",
    "    trigger_idx = random.choice(range(len(triggerList)))\n",
    "    trigger = triggerList[trigger_idx].to(args.device)\n",
    "    graph_idx = list(range(len(training_set)))\n",
    "    start = time.time()\n",
    "    while len(graph_idx) > 0:\n",
    "        cur_batch = random.choices(graph_idx, k=batch_size)\n",
    "        graph_idx = [tmp for tmp in graph_idx if tmp not in cur_batch]\n",
    "        modi_graph = BatchAppend([training_set[i].to(args.device)\n",
    "                                 for i in cur_batch], trigger, pos_mode)\n",
    "        modi_graph.x.requires_grad = True\n",
    "        embed_adv = model(modi_graph)\n",
    "\n",
    "        ori_graph = geo.data.Batch.from_data_list(\n",
    "            [training_set[i].to(args.device) for i in cur_batch]).to(args.device)\n",
    "        ori_graph.x.requires_grad = True\n",
    "\n",
    "        embed_ori = model(ori_graph)\n",
    "\n",
    "        labels = geo.data.Batch.from_data_list(\n",
    "            [training_set[i].to(args.device) for i in cur_batch]).y\n",
    "\n",
    "        output = getOutput(embed_adv)\n",
    "        loss = F.nll_loss(output, labels)\n",
    "        grad_embed_adv = torch.autograd.grad(\n",
    "            loss, embed_adv)  # , allow_unused=True)\n",
    "\n",
    "        output = getOutput(embed_ori)\n",
    "        loss = F.nll_loss(output, labels)\n",
    "        grad_embed_ori = torch.autograd.grad(\n",
    "            loss, embed_ori)  # , allow_unused=True)\n",
    "\n",
    "        cur_score = torch.ones((len(triggerList, )))\n",
    "        for triggerCand in range(len(triggerList)):\n",
    "            embed_others = model(BatchAppend(\n",
    "                [training_set[i].to(args.device) for i in cur_batch],\n",
    "                triggerList[triggerCand],\n",
    "                pos_mode\n",
    "            ).to(args.device))\n",
    "            cur_score[triggerCand] = torch.matmul((embed_others - embed_adv).mean(dim=0).unsqueeze(0),\n",
    "                                                  (grad_embed_ori[0] - grad_embed_adv[0]).mean(dim=0).unsqueeze(-1)).item()\n",
    "        trigger_idx = cur_score.argmin(-1)\n",
    "    time_used = time.time() - start\n",
    "    modi_graph = BatchAppend(test_set, triggerList[trigger_idx], pos_mode)\n",
    "    embed_adv = model(modi_graph)\n",
    "    output = getOutput(embed_adv)\n",
    "    accu = (output.argmax(-1) ==\n",
    "            geo.data.Batch.from_data_list(test_set).y).int().sum().item() / len(test_set)\n",
    "    log['trigger_idx'] = trigger_idx.item()\n",
    "    log['accu'] = accu\n",
    "    log['timeCost'] = time_used\n",
    "    result[pos_mode] = log\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "source": [
    "result"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'length: 1 mode: deg': {'trigger_idx': [82],\n",
       "  'accu': 0.5714285714285714,\n",
       "  'timeCost': 992.6306567192078},\n",
       " 'length: 1 mode: min': {'trigger_idx': [106],\n",
       "  'accu': 0.5357142857142857,\n",
       "  'timeCost': 946.7149651050568},\n",
       " 'length: 1 mode: btw': {'trigger_idx': [80],\n",
       "  'accu': 0.7410714285714286,\n",
       "  'timeCost': 938.4616959095001},\n",
       " 'length: 1 mode: eig': {'trigger_idx': [106],\n",
       "  'accu': 0.5892857142857143,\n",
       "  'timeCost': 938.4162719249725},\n",
       " 'length: 2 mode: deg': {'trigger_idx': [106, 106],\n",
       "  'accu': 0.5178571428571429,\n",
       "  'timeCost': 936.9213075637817},\n",
       " 'length: 2 mode: min': {'trigger_idx': [106, 106],\n",
       "  'accu': 0.5535714285714286,\n",
       "  'timeCost': 936.5684282779694},\n",
       " 'length: 2 mode: btw': {'trigger_idx': [106, 106],\n",
       "  'accu': 0.5714285714285714,\n",
       "  'timeCost': 937.3475587368011},\n",
       " 'length: 2 mode: eig': {'trigger_idx': [106, 106],\n",
       "  'accu': 0.5357142857142857,\n",
       "  'timeCost': 934.8087739944458},\n",
       " 'length: 3 mode: deg': {'trigger_idx': [106, 106, 106],\n",
       "  'accu': 0.5803571428571429,\n",
       "  'timeCost': 955.478107213974},\n",
       " 'length: 3 mode: min': {'trigger_idx': [106, 106, 106],\n",
       "  'accu': 0.4732142857142857,\n",
       "  'timeCost': 954.6713514328003},\n",
       " 'length: 3 mode: btw': {'trigger_idx': [106, 106, 106],\n",
       "  'accu': 0.5892857142857143,\n",
       "  'timeCost': 954.6159300804138},\n",
       " 'length: 3 mode: eig': {'trigger_idx': [82, 106, 82],\n",
       "  'accu': 0.6428571428571429,\n",
       "  'timeCost': 957.419516324997}}"
      ]
     },
     "metadata": {},
     "execution_count": 43
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('graphEmbeddingResult.json', 'w+') as fout:\n",
    "    json.dump(result, fout)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "ori_graph = geo.data.Batch.from_data_list(test_set).to(args.device)\n",
    "embed_ori = model(ori_graph)\n",
    "output = getOutput(embed_ori)\n",
    "(output.argmax(-1) == geo.data.Batch.from_data_list(test_set).y).int().sum().item() / len(test_set)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### experiment records:\n",
    "* hotflip attack embedding difference version: \n",
    "    * to max degree, 76.19% accu\n",
    "    * to lowest degree, 75.29% accu"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## new idea: use another similarity metric rather than difference, use node feature as embedding (variable trigger length)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### just replace embedding:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = {}\n",
    "for triggerLen in range(1, 4):\n",
    "    for pos_mode in pos_modes:\n",
    "        start = time.time()\n",
    "        log = {}\n",
    "        graph_idx = list(range(len(training_set)))\n",
    "        trigger_idx = random.choices(range(len(triggerList)), k=triggerLen)\n",
    "        while len(graph_idx) > 0:\n",
    "            trigger = getTrigger(n=triggerLen, idx=trigger_idx)\n",
    "            cur_batch = random.choices(graph_idx, k=batch_size)\n",
    "            graph_idx = [tmp for tmp in graph_idx if tmp not in cur_batch]\n",
    "            modi_graph = BatchAppend([training_set[i] for i in cur_batch], trigger, pos_mode)\n",
    "            modi_graph.x.requires_grad_()\n",
    "            embed_adv = model(modi_graph)\n",
    "\n",
    "            ori_graph = geo.data.Batch.from_data_list([training_set[i] for i in cur_batch]).to(args.device)\n",
    "            ori_graph.x.requires_grad_()\n",
    "\n",
    "            embed_ori = model(ori_graph)\n",
    "\n",
    "            labels = geo.data.Batch.from_data_list([training_set[i] for i in cur_batch]).y\n",
    "\n",
    "            output = getOutput(embed_adv)\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            grad_embed_adv = torch.autograd.grad(loss, modi_graph.x)#, allow_unused=True)\n",
    "\n",
    "            output = getOutput(embed_ori)\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            grad_embed_ori = torch.autograd.grad(loss, ori_graph.x)#, allow_unused=True)\n",
    "\n",
    "            for i in range(len(trigger_idx)):\n",
    "                cur_score = torch.ones((len(triggerList, )))\n",
    "                for triggerCand in range(len(triggerList)):\n",
    "                    cur_score[triggerCand] = torch.matmul((triggerList[triggerCand].x - modi_graph.x[-(triggerLen-i)]).mean(dim=0).unsqueeze(0),\n",
    "                                                          (grad_embed_adv[0][-(triggerLen-i)]).unsqueeze(-1)).item()\n",
    "                trigger_idx[i] = cur_score.argmin(-1).item()\n",
    "        time_used = time.time() - start\n",
    "        trigger = getTrigger(n=triggerLen, idx=trigger_idx)\n",
    "        modi_graph = BatchAppend(test_set, trigger, pos_mode)\n",
    "        embed_adv = model(modi_graph)\n",
    "        output = getOutput(embed_adv)\n",
    "        accu = (output.argmax(-1) == geo.data.Batch.from_data_list(test_set).y).int().sum().item() / len(test_set)\n",
    "        log['trigger_idx'] = trigger_idx\n",
    "        log['accu'] = accu\n",
    "        log['timeCost'] = time_used\n",
    "        result['length: {} mode: {}'.format(triggerLen, pos_mode)] = log"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('nodeEmbeddingResult.json', 'w+') as fout:\n",
    "    json.dump(result, fout)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### records:\n",
    "* benign: 77.89%\n",
    "* length=1: 63.61% (lowest degree), \n",
    "* length=2: 66.49%\n",
    "* length=3: 65.41%"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# for i in range(len(list(cands.keys()))):\n",
    "#     trigger = getTrigger(n=1, idx=[i])\n",
    "#     modi_graph = BatchAppend(dataset, trigger)\n",
    "#     embed_adv = model(modi_graph)\n",
    "#     output = getOutput(embed_adv)\n",
    "#     accu = (output.argmax(-1) == geo.data.Batch.from_data_list(dataset).y).int().sum().item() / len(dataset)\n",
    "#     print(i, accu)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# len(list(cands.keys()))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## on surrogate data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "result = {}\n",
    "for triggerLen in range(1, 4):\n",
    "    for pos_mode in pos_modes:\n",
    "        log = {}\n",
    "        start = time.time()\n",
    "        graph_idx = list(range(len(cldataList)))\n",
    "        trigger_idx = random.choices(range(len(triggerList)), k=triggerLen)\n",
    "        while len(graph_idx) > 0:\n",
    "            trigger = getTrigger(n=triggerLen, idx=trigger_idx, weighted=True)\n",
    "            cur_batch = random.choices(graph_idx, k=batch_size)\n",
    "            graph_idx = [tmp for tmp in graph_idx if tmp not in cur_batch]\n",
    "            modi_graph = BatchAppend([cldataList[i] for i in cur_batch], trigger)\n",
    "            modi_graph.x.requires_grad_()\n",
    "            embed_adv = model(modi_graph)\n",
    "\n",
    "            ori_graph = geo.data.Batch.from_data_list([cldataList[i] for i in cur_batch]).to(args.device)\n",
    "            ori_graph.x.requires_grad_()\n",
    "\n",
    "            embed_ori = model(ori_graph)\n",
    "\n",
    "            labels = geo.data.Batch.from_data_list([cldataList[i] for i in cur_batch]).y\n",
    "\n",
    "            output = getOutput(embed_adv)\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            grad_embed_adv = torch.autograd.grad(loss, modi_graph.x)#, allow_unused=True)\n",
    "\n",
    "            output = getOutput(embed_ori)\n",
    "            loss = F.nll_loss(output, labels)\n",
    "            grad_embed_ori = torch.autograd.grad(loss, ori_graph.x)#, allow_unused=True)\n",
    "\n",
    "            for i in range(len(trigger_idx)):\n",
    "                cur_score = torch.ones((len(triggerList, )))\n",
    "                for triggerCand in range(len(triggerList)):\n",
    "                    cur_score[triggerCand] = torch.matmul((triggerList[triggerCand].x - modi_graph.x[-(triggerLen-i)]).mean(dim=0).unsqueeze(0),\n",
    "                                                          (grad_embed_adv[0][-(triggerLen-i)]).unsqueeze(-1)).item()\n",
    "                trigger_idx[i] = cur_score.argmin(-1).item()\n",
    "        time_used = time.time() - start\n",
    "        trigger = getTrigger(n=triggerLen, idx=trigger_idx, weighted=True)\n",
    "        modi_graph = BatchAppend(test_set, trigger, pos_mode)\n",
    "        embed_adv = model(modi_graph)\n",
    "        output = getOutput(embed_adv)\n",
    "        accu = (output.argmax(-1) == geo.data.Batch.from_data_list(test_set).y).int().sum().item() / len(test_set)\n",
    "        log['trigger_idx'] = trigger_idx\n",
    "        log['accu'] = accu\n",
    "        log['timeCost'] = time_used\n",
    "        result['length: {} mode: {}'.format(triggerLen, pos_mode)] = log"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "(grad_embed_adv[0][-(triggerLen-i)]).unsqueeze(-1).shape, (triggerList[triggerCand].x - modi_graph.x[-(triggerLen-i)]).mean(dim=0).unsqueeze(0).shape"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "triggerLen"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "min([tmp.x.shape[0] for tmp in cldataList])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "with open('nodeEmbeddingResult_ClsImprs.json', 'w+') as fout:\n",
    "    json.dump(result, fout)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "fileNames = [tmp for tmp in os.listdir() if tmp.endswith('.json') and 'Result' in tmp]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "overallResult = pd.DataFrame()\n",
    "for fileName in fileNames:\n",
    "    with open(fileName, 'r') as fin:\n",
    "        tmp = json.load(fin)\n",
    "    for key in list(tmp.keys()):\n",
    "        overallResult = overallResult.append(pd.DataFrame({tmpKey: str(tmp[key][tmpKey]) for tmpKey in list(tmp[key].keys())}, \n",
    "                                index=['{} {}'.format(fileName.split('.')[0], key)]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "overallResult.to_csv('OverallResult_updated.csv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "source": [
    "result = {}\n",
    "graph_idx = list(range(len(cldataList)))\n",
    "for triggerLen in range(1, 4):\n",
    "    for pos_mode in pos_modes:\n",
    "        log = {}\n",
    "        tmp = 0\n",
    "        for i in range(5): # repeat several times\n",
    "            trigger_idx = random.choices(range(len(triggerList)), k=triggerLen)\n",
    "            trigger = getTrigger(n=triggerLen, idx=trigger_idx, weighted=True)\n",
    "            modi_graph = BatchAppend(test_set, trigger, pos_mode)\n",
    "            embed_adv = model(modi_graph)\n",
    "            output = getOutput(embed_adv)\n",
    "            tmp += (output.argmax(-1) == geo.data.Batch.from_data_list(test_set).y).int().sum().item() / len(test_set)\n",
    "        log['accu'] = tmp / 5.0\n",
    "        result['length: {} mode: {}'.format(triggerLen, pos_mode)] = log"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "source": [
    "pd.DataFrame(result).to_csv('random_baseline.csv')"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}